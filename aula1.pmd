# E1 - An√°lise de dados e Regress√£o Linear: Pre√ßo de im√≥veis

Considere a base de dados "Ames Housing Dataset". Com o objetivo de desenvolver um modelo de predi√ß√£o do pre√ßo do im√≥vel, desenvolva os itens a seguir e entregue a an√°lise em arquivo do tipo powerpoint ou pdf

- Importando as bibliotecas
```python
import pandas as pd
import numpy as np
import statsmodels
import seaborn
from matplotlib import pyplot as plt
pd.options.display.max_columns = 100
```
- Importando a base de dados
```python
df = pd.read_csv('base_1ah.csv')
print(df.shape)
```

- Visualizando uma amostra da base_1ah
```python
 df.head()
```
```python
 df = df.set_index('Id')
```

### 1 - An√°lise descritiva de vari√°veis

1.1 Estat√≠sticas descritivas: frequ√™ncia, propor√ß√£o, m√©dia (ùë•¬Ø), desvio padr√£o (ùë†), quartis (ùëÑ1, ùë•ÃÉ , ùëÑ3)      (1,0)

Avaliando o tipo das vari√°veis na base_1ah
```python
 df.info()
```

An√°lise descritiva para vari√°veis num√©ricas
```python
 df.describe()
```
An√°lise descritiva das vari√°veis categ√≥ricas
```python
df.describe(include=object)
```
```python
for i, x in enumerate(df.dtypes):
  if x == 'object':
    print(pd.crosstab(index=df[df.columns[i]], columns='freq', dropna=False))
    print('')
```
1.2 Gr√°ficos como: Gr√°ficos de colunas, BoxPlot, dispers√£o          (2,0)

Para gr√°ficos num√©ricos plotamos boxplot e para categ√≥ricos gr√°ficos de barra.
```python
for i, x in enumerate(df.dtypes):
   if x == 'int64' or x == 'float64':
       plt.figure(i)
       seaborn.boxplot(y = df[df.columns[i]])
   elif x == 'int64' or x == 'object':
       plt.figure(i)
       seaborn.barplot(x = df[df.columns[i]], y = range(0,len(df)))
```
Gr√°ficos de dispers√£o para dados num√©ricos
```python
for i, x in enumerate(df.dtypes):
    if x == 'float64':
        plt.figure(i)
        seaborn.scatterplot(x = df.SalePrice ,y = df[df.columns[i]])
```
2 - An√°lise de correla√ß√µes (ùëüùë•ùëñ,ùë•ùëó)

2.1 Correlograma                                                  (1,5)
```python
df_corr = df.corr()

import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(20, 12))
# mask
mask = np.triu(np.ones_like(df_corr, dtype=np.bool))
# adjust mask and df
mask = mask[1:, :-1]
corr = df_corr.iloc[1:,:-1].copy()
# plot heatmap
seaborn.heatmap(corr, mask=mask, annot=True, fmt=".2f", cmap='Blues',
           vmin=-1, vmax=1, cbar_kws={"shrink": .8})
# yticks
plt.yticks(rotation=0)
plt.show()
```
2.2 An√°lise sobre correla√ß√µes significativas     (1,5)

Correla√ß√µes com a vari√°vel resposta
```python
df.corr().loc['SalePrice']
```
Podemos notar que as vari√°veis com maior correla√ß√£o s√£o:

-OverallQual      0.790982 - Faz todo sentido, dado que aqui √© uma nota que d√£o para o im√≥vel

-TotalBsmtSF      0.613581 - Talvez fa√ßa sentido o tamanho do sot√£o, pois existe a possibilidade de virar um espa√ßo para alugar

-1stFlrSF         0.605852 - Aqui quanto maior a metragem do primeiro andar, maior a √°rea comum, logo um valor maior

-GrLivArea        0.708624 - Idem ao de cima, tamanho maior da √°rea externa de conviv√™ncia

-GarageCars       0.640409 - Quantidade de carro que cabem na garagem valorizam o im√≥vel

-GarageArea       0.623431 - Muita correla√ß√£o com o de cima (0.88)

3 - Desenvolvimento de modelo de Regress√£o utilizando Regress√£o Linear com o m√©todo de m√≠nimos quadrados ordin√°rios. Apresente as caracter√≠sticas do desenvolvimento: amostras, medidas de avalia√ß√£o do  modelo...

Primeiramente realizamos um tratamento nas variaveis categ√≥ricas e nos missings
```python
categorical_data = ['MSSubClass','MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig',
'LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st',
'Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',
'BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu','GarageType',
'GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition']

num_data = ['LotFrontage','LotArea','OverallQual','OverallCond','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',
'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',
'BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',
'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal']

drop_data = ['id']

date_data = ['YearBuilt','YearRemodAdd','GarageYrBlt','MoSold','YrSold']

Y = df.SalePrice

X_cat_df = pd.get_dummies(df[categorical_data].fillna('NA'))

X_num_data = df[num_data].fillna(0)

df['garageTime'] = df.YrSold - df.GarageYrBlt

df['timeToSell'] = df.YrSold - df.YearBuilt

X = pd.concat([X_cat_df, X_num_data, df.garageTime.fillna(0), df.timeToSell.fillna(0)], axis = 1)
```
Importando as bibliotecas de ML
```python
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
```

Aplicamos uma taxa de amostragem para teste de 30%, deixando 70% para treino
```python
 X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1234)
```
Para selecionar as variaveis mais relevantes fitamos uma radom forest e selecionamos as 20 variaveis mais relevantes.
```python
rf = RandomForestRegressor(n_estimators=900,n_jobs=-1, max_depth=5)
rf.fit(X_train,y_train)
best_feat = list(pd.DataFrame(rf.feature_importances_, index=X_train.columns,columns=['Importance']).sort_values('Importance', ascending = False).index[0:20])
best_feat
```

```python
lr = LinearRegression()
lr.fit(X_train[best_feat],y_train)
```
```python
print('Test R2 =', r2_score(y_test, lr.predict(X_test[best_feat])))
print('Train R2 =', r2_score(y_train, lr.predict(X_train[best_feat])))
```

```python
print('Test MAE =', mean_absolute_error(y_test, lr.predict(X_test[best_feat])))
print('Train MAE =', mean_absolute_error(y_train, lr.predict(X_train[best_feat])))
```

```python
print('Test RMSE =', np.sqrt(mean_squared_error(y_test, lr.predict(X_test[best_feat]))))
print('Train RMSE =', np.sqrt(mean_squared_error(y_train, lr.predict(X_train[best_feat]))))
```
Importando as bibliotecas para o teste de ANOVA
```python
from statsmodels.api import OLS, add_constant,graphics
from scipy import stats
```


Avaliando a significancia dos coeficientes e analise de res√≠duos:
```python
X2_train = add_constant(X_train[best_feat])
est = OLS(y_train, X2_train)
est2 = est.fit()
print(est2.summary())

seaborn.residplot(lr.predict(X_test[best_feat]),y_test, lowess=True,
                                  line_kws={'color': 'red', 'lw': 1, 'alpha': 1})
plt.xlabel("Fitted values")
plt.title('Residual plot')
```
```python
residuals = y_test - lr.predict(X_test[best_feat])
residuals

plt.figure(figsize=(7,7))
stats.probplot(residuals, dist="norm", plot=plt)
plt.title("Normal Q-Q Plot")

```

```python
model_norm_residuals_abs_sqrt=np.sqrt(np.abs(residuals))

plt.figure(figsize=(7,7))
seaborn.regplot(lr.predict(X_test[best_feat]), model_norm_residuals_abs_sqrt,
              scatter=True,
              lowess=True,
              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})
plt.ylabel("Standarized residuals")
plt.xlabel("Fitted value")
plt.title('Scale-Location plot')
```
```python
from scipy import stats
shapiro_test = stats.shapiro(residuals)
print(shapiro_test)
```

## Conclus√µes
  Analisando o R2 podemos verificar que o modelo explica 78% da vari√°vel resposta, ou seja, as variaveis escolhidas explicam 78% do pre√ßo da casa.
  Avaliando a Prob-F, dado que ela √© menor que 0.05 podemos rejeitar a hip√≥tese H0 de que a regress√£o linear n√£o existe.
  Na m√©trica de Durbin-Watson tambem podemos concluir que n√£o a correla√ß√£o entre os res√≠duos.
  Apenas com estas m√©tricas poderiamos concluir que temos um bom modelo, por√©m ao continuar nossas an√°lises podemos notar que tanto MAE e RMSE apresentam valores significantes e a diferen√ßa entre o RMSE e MAE nos permite supor que possa existir algum outlier e que os erros n√£o s√£o uniformes.
  Pela analise de res√≠duo podemos verificar graficamente que os erros n√£o s√£o uniformemente distribuidos e que apresentam um leve formato de U. Aplicando o teste de shapiro com o resultado de p-value<0.05, isso nos leva a aceitar a hipotese nula de que os erros n√£o seguem uma distribui√ß√£o normal. O mesmo resultado pode ser obtido pela Prob Omnibus apresentada na tabela OLS Regression Results.
  Portanto, podemos concluir que a regress√£o linear n√£o √© a melhor escolha.
