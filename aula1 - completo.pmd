# E1 - An√°lise de dados e Regress√£o Linear: Pre√ßo de im√≥veis

Considere a base de dados "Ames Housing Dataset". Com o objetivo de desenvolver um modelo de predi√ß√£o do pre√ßo do im√≥vel, desenvolva os itens a seguir e entregue a an√°lise em arquivo do tipo powerpoint ou pdf

- Importando as bibliotecas
```python
import pandas as pd
import numpy as np
import statsmodels
import seaborn
from matplotlib import pyplot as plt
pd.options.display.max_columns = 100
```
- Importando a base de dados
```python
df = pd.read_csv('base_1ah.csv')
print(df.shape)
```

- Visualizando uma amostra da base_1ah
```python
 df.head()
```
```python
 df = df.set_index('Id')
```

### 1 - An√°lise descritiva de vari√°veis

1.1 Estat√≠sticas descritivas: frequ√™ncia, propor√ß√£o, m√©dia (ùë•¬Ø), desvio padr√£o (ùë†), quartis (ùëÑ1, ùë•ÃÉ , ùëÑ3)      (1,0)

Avaliando o tipo das vari√°veis na base_1ah
```python
 df.info()
```

An√°lise descritiva para vari√°veis num√©ricas
```python
 df.describe()
```
An√°lise descritiva das vari√°veis categ√≥ricas
```python
df.describe(include=object)
```
```python
for i, x in enumerate(df.dtypes):
  if x == 'object':
    print(pd.crosstab(index=df[df.columns[i]], columns='freq', dropna=False))
    print('')
```
1.2 Gr√°ficos como: Gr√°ficos de colunas, BoxPlot, dispers√£o          (2,0)

Para gr√°ficos num√©ricos plotamos boxplot e para categ√≥ricos gr√°ficos de barra.
```python
for i, x in enumerate(df.dtypes):
   if x == 'int64' or x == 'float64':
       plt.figure(i)
       seaborn.boxplot(y = df[df.columns[i]])
   elif x == 'int64' or x == 'object':
       plt.figure(i)
       seaborn.barplot(x = df[df.columns[i]], y = range(0,len(df)))
```
Gr√°ficos de dispers√£o para dados num√©ricos
```python
for i, x in enumerate(df.dtypes):
    if x == 'float64':
        plt.figure(i)
        seaborn.scatterplot(x = df.SalePrice ,y = df[df.columns[i]])
```
2 - An√°lise de correla√ß√µes (ùëüùë•ùëñ,ùë•ùëó)

2.1 Correlograma                                                  (1,5)
```python
df_corr = df.corr()

import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(20, 12))
# mask
mask = np.triu(np.ones_like(df_corr, dtype=np.bool))
# adjust mask and df
mask = mask[1:, :-1]
corr = df_corr.iloc[1:,:-1].copy()
# plot heatmap
seaborn.heatmap(corr, mask=mask, annot=True, fmt=".2f", cmap='Blues',
           vmin=-1, vmax=1, cbar_kws={"shrink": .8})
# yticks
plt.yticks(rotation=0)
plt.show()
```
2.2 An√°lise sobre correla√ß√µes significativas     (1,5)

Correla√ß√µes com a vari√°vel resposta
```python
df.corr().loc['SalePrice']
```
Podemos notar que as vari√°veis com maior correla√ß√£o s√£o:

-OverallQual      0.790982 - Faz todo sentido, dado que aqui √© uma nota que d√£o para o im√≥vel

-TotalBsmtSF      0.613581 - Talvez fa√ßa sentido o tamanho do sot√£o, pois existe a possibilidade de virar um espa√ßo para alugar

-1stFlrSF         0.605852 - Aqui quanto maior a metragem do primeiro andar, maior a √°rea comum, logo um valor maior

-GrLivArea        0.708624 - Idem ao de cima, tamanho maior da √°rea externa de conviv√™ncia

-GarageCars       0.640409 - Quantidade de carro que cabem na garagem valorizam o im√≥vel

-GarageArea       0.623431 - Muita correla√ß√£o com o de cima (0.88)

3 - Desenvolvimento de modelo de Regress√£o utilizando Regress√£o Linear com o m√©todo de m√≠nimos quadrados ordin√°rios. Apresente as caracter√≠sticas do desenvolvimento: amostras, medidas de avalia√ß√£o do  modelo...

Primeiramente realizamos um tratamento nas variaveis categ√≥ricas e nos missings
```python
categorical_data = ['MSSubClass','MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig',
'LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st',
'Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',
'BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu','GarageType',
'GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition']

num_data = ['LotFrontage','LotArea','OverallQual','OverallCond','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',
'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',
'BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',
'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal']

drop_data = ['id']

date_data = ['YearBuilt','YearRemodAdd','GarageYrBlt','MoSold','YrSold']

Y = df.SalePrice

X_cat_df = pd.get_dummies(df[categorical_data].fillna('NA'))

X_num_data = df[num_data].fillna(0)

df['garageTime'] = df.YrSold - df.GarageYrBlt

df['timeToSell'] = df.YrSold - df.YearBuilt

X = pd.concat([X_cat_df, X_num_data, df.garageTime.fillna(0), df.timeToSell.fillna(0)], axis = 1)
```
Importando as bibliotecas de ML
```python
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
```

Aplicamos uma taxa de amostragem para teste de 30%, deixando 70% para treino
```python
 X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1234)
```
maneira 1: sele√ßao de variaveis pelas com maior correla√ßao
```python
temp = pd.concat([Y,X], axis=1).corr().loc['SalePrice']
temp = temp.reindex(temp.abs().sort_values(ascending=False).index)[1:21]
best_feat2 = list(temp.index)
best_feat2
```
```python
lr2 = LinearRegression()
lr2.fit(X_train[best_feat2],y_train)
```

```python
print('Test R2 =', r2_score(y_test, lr2.predict(X_test[best_feat2])))
print('Train R2 =', r2_score(y_train, lr2.predict(X_train[best_feat2])))

```
```python
print('Test RMSE =', np.sqrt(mean_squared_error(y_test, lr2.predict(X_test[best_feat2]))))
print('Train RMSE =', np.sqrt(mean_squared_error(y_train, lr2.predict(X_train[best_feat2]))))
```
```python
print('Test MAE =', mean_absolute_error(y_test, lr2.predict(X_test[best_feat2])))
print('Train MAE =', mean_absolute_error(y_train, lr2.predict(X_train[best_feat2])))

```
maneira 2: Para selecionar as variaveis mais relevantes fitamos uma radom forest e selecionamos as 20 variaveis mais relevantes.
```python
rf = RandomForestRegressor(n_estimators=900,n_jobs=-1, max_depth=5)
rf.fit(X_train,y_train)
best_feat = list(pd.DataFrame(rf.feature_importances_, index=X_train.columns,columns=['Importance']).sort_values('Importance', ascending = False).index[0:20])
best_feat
```

```python
lr = LinearRegression()
lr.fit(X_train[best_feat],y_train)
```
```python
print('Test R2 =', r2_score(y_test, lr.predict(X_test[best_feat])))
print('Train R2 =', r2_score(y_train, lr.predict(X_train[best_feat])))
```

```python
print('Test MAE =', mean_absolute_error(y_test, lr.predict(X_test[best_feat])))
print('Train MAE =', mean_absolute_error(y_train, lr.predict(X_train[best_feat])))
```

```python
print('Test RMSE =', np.sqrt(mean_squared_error(y_test, lr.predict(X_test[best_feat]))))
print('Train RMSE =', np.sqrt(mean_squared_error(y_train, lr.predict(X_train[best_feat]))))
```
maneira 3: Para selecionar as variaveis mais relevantes utilizamos a fun√ß√£o SelectKBest e selecionamos as 20 variaveis mais relevantes.
```python
from sklearn.feature_selection import SelectKBest, f_regression, chi2
fs = SelectKBest(score_func=f_regression, k=20)
X_selected = fs.fit_transform(X, Y)

cols = fs.get_support()
names = X.columns.values[cols]
scores = fs.scores_[cols]
names_scores = list(zip(names, scores))

names_scores
```

```python
lr3 = LinearRegression()
lr3.fit(X_train[names],y_train)
```
```python
print('Test R2 =', r2_score(y_test, lr3.predict(X_test[names])))
print('Train R2 =', r2_score(y_train, lr3.predict(X_train[names])))
```

```python
print('Test MAE =', mean_absolute_error(y_test, lr3.predict(X_test[names])))
print('Train MAE =', mean_absolute_error(y_train, lr3.predict(X_train[names])))
```

```python
print('Test RMSE =', np.sqrt(mean_squared_error(y_test, lr3.predict(X_test[names]))))
print('Train RMSE =', np.sqrt(mean_squared_error(y_train, lr3.predict(X_train[names]))))
```


```python
from statsmodels.api import OLS, add_constant,graphics
from scipy import stats
```


Avaliando a significancia dos coeficientes e analise de res√≠duos:
```python
X2_train = add_constant(X_train[best_feat])
est = OLS(y_train, X2_train)
est2 = est.fit()
print(est2.summary())

seaborn.residplot(lr.predict(X_test[best_feat]),y_test, lowess=True,
                                  line_kws={'color': 'red', 'lw': 1, 'alpha': 1})
plt.xlabel("Fitted values")
plt.title('Residual plot')
```
```python
residuals = y_test - lr.predict(X_test[best_feat])
residuals

plt.figure(figsize=(7,7))
stats.probplot(residuals, dist="norm", plot=plt)
plt.title("Normal Q-Q Plot")

```

```python


model_norm_residuals_abs_sqrt=np.sqrt(np.abs(residuals))

plt.figure(figsize=(7,7))
seaborn.regplot(lr.predict(X_test[best_feat]), model_norm_residuals_abs_sqrt,
              scatter=True,
              lowess=True,
              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})
plt.ylabel("Standarized residuals")
plt.xlabel("Fitted value")
plt.title('Scale-Location plot')
```
```python
from scipy import stats
shapiro_test = stats.shapiro(residuals)
print(shapiro_test)
```
```python
import matplotlib.pyplot as plt
plt.scatter(lr.predict(X_test[best_feat]), residuals)
plt.show()
```
```python
#modelo1
X2_train = add_constant(X_train[best_feat2])
est = OLS(y_train, X2_train)
est2 = est.fit()
print(est2.summary())

seaborn.residplot(lr.predict(X_test[best_feat2]),y_test, lowess=True,
                                  line_kws={'color': 'red', 'lw': 1, 'alpha': 1})
plt.xlabel("Fitted values")
plt.title('Residual plot')
```

```python
 new_list = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'BsmtQual_Ex','timeToSell','KitchenQual_TA',
 'KitchenQual_Ex','MasVnrArea', 'Fireplaces', 'ExterQual_Ex']

 lr4 = LinearRegression()
 lr4.fit(X_train[new_list],y_train)

 X2_train = add_constant(X_train[new_list])
 est = OLS(y_train, X2_train)
 est2 = est.fit()
 print(est2.summary())

 seaborn.residplot(lr4.predict(X_test[new_list]),y_test, lowess=True,
                                   line_kws={'color': 'red', 'lw': 1, 'alpha': 1})
 plt.xlabel("Fitted values")
 plt.title('Residual plot')

```
